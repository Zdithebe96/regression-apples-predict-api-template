{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLE GOLDEN DELICIOUS PREDICTION\n",
    "***\n",
    "\n",
    "Our task is to predict the average price per apple of APPLE GOLDEN DELICIOUS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GETTING STARTED\n",
    "\n",
    "Importing necessary libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import scipy\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "\n",
    "train = pd.read_csv('df - train_set.csv')\n",
    "test = pd.read_csv('df - test_set.csv')\n",
    "\n",
    "train = train[(train['Commodities'] == 'APPLE GOLDEN DELICIOUS')]\n",
    "del train['Commodities']\n",
    "del test['Commodities']\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPLORATORY DATA ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting info\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Province'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Container'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Size_Grade'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pairplot**\n",
    "\n",
    "Pairplot  to visualize the relationship between numeric and categorical data.\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heatmap**\n",
    "\n",
    "We used a heatmap to understand correlation between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train.corr(), annot=True, cmap='magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- According to the above correlation matrix Low price, High price, Sales total, Total kg sold, Stock on hand,Total Qty sold, are highly correlated. \n",
    "- Highly correlated variables will cause the model to be biased towards them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution Plot**\n",
    "\n",
    "Understanding the distribution of our target \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train['avg_price_per_kg'],kde =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesing Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(df, column):\n",
    "    df = df.copy()\n",
    "    dummies = pd.get_dummies(df[column], prefix=column, drop_first=True)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_inputs(df, return_df=False):\n",
    "    df = df.copy()\n",
    "\n",
    "    # CLEAN PROVINCE COLUMN\n",
    "\n",
    "    df['Province'] = df['Province'].str.replace(' ', '_')\n",
    "    df['Province'] = df['Province'].str.replace('.', '_')\n",
    "    df['Province'] = df['Province'].str.replace('-', '_')\n",
    "\n",
    "    # DATE ENCODING\n",
    "    # Split 'Date' column into year, month and day columns\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Date'] = df['Date'].dt.strftime('%d.%m.%Y')\n",
    "    df['year'] = pd.DatetimeIndex(df['Date']).year\n",
    "    df['month'] = pd.DatetimeIndex(df['Date']).month\n",
    "    df['day'] = pd.DatetimeIndex(df['Date']).day\n",
    "\n",
    "    df = df.drop(['Date'], axis=1)\n",
    "    # BINARY ENCODING\n",
    "\n",
    "    df['year'] = df['year'].replace({2020: 1, 2019: 0})\n",
    "\n",
    "    # ONE-HOT ENCODING\n",
    "    for column in ['Province', 'Container']:\n",
    "        df = onehot_encode(df, column)\n",
    "\n",
    "    # ORDINAL ENCODING\n",
    "    df['Size_Grade'] = df['Size_Grade'].replace({\n",
    "        '1X': 9,\n",
    "        '1L': 8,\n",
    "        '1M': 7,\n",
    "        '1S': 6,\n",
    "        '1U': 0,\n",
    "        '2X': 3,\n",
    "        '2L': 5,\n",
    "        '2M': 4,\n",
    "        '2S': 1,\n",
    "        '2U': 2\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets check for outliers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessing_inputs(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = X.drop('avg_price_per_kg', axis=1).copy()\n",
    "\n",
    "categorical_columns = [\n",
    "    column for column in out_df.columns if len(out_df[column].unique()) > 2\n",
    "]\n",
    "# Thisisto make sure we dont use categorical data\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "for i, column in enumerate(categorical_columns):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    sns.boxplot(data=out_df[column].values, color='darkviolet')\n",
    "    plt.title(column)\n",
    "\n",
    "plt.suptitle(\"Boxplots With Outliers\", size=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see from the above boxplot the following features (weight kg, low price, high price,sales total, total qty sold,   total kg sold, stock on hand) may have possible outliers.\n",
    "- We will use Z scores for our analysis\n",
    "- Any feature that lies outside a z score threshold will be considered an outlier \n",
    "- Making the assumption that the variable follows a normal distribution\n",
    "- The further away it is from 0( i.e mean) the more extreme it is ,the more likely it is to be an outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, columns, threshold, asxis=0):\n",
    "    df = df.copy()\n",
    "    # Calculate the lower and upper bounds on the Z distribution given a threshold value\n",
    "    lower_bound = scipy.stats.norm.ppf(q=threshold / 2, loc=0, scale=1)\n",
    "    upper_bound = scipy.stats.norm.ppf(q=1 - threshold / 2, loc=0, scale=1)\n",
    "\n",
    "    # Calculate X scores for affected columns\n",
    "    outlier_df = outlier_df = df.loc[:, columns].copy()\n",
    "    zscores = pd.DataFrame(scipy.stats.zscore(outlier_df, axis=0),\n",
    "                           index=outlier_df.index,\n",
    "                           columns=outlier_df.columns)\n",
    "\n",
    "    # Get boolean arrays denoting the outlier examples\n",
    "    lower_outliers = (zscores < lower_bound).any(axis=1)\n",
    "    upper_outliers = (zscores >= upper_bound).any(axis=1)\n",
    "\n",
    "    # Get indicies of all outlier examples\n",
    "    outliers = df[pd.concat([lower_outliers, upper_outliers],\n",
    "                            axis=1).any(axis=1)].index\n",
    "\n",
    "    # Drop the outliers\n",
    "    df = df.drop(outliers, axis=0).reset_index(drop=True)\n",
    "    #print(len(outliers), \"examples dropped.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "seed = 1\n",
    "\n",
    "\n",
    "def preprocess_inputs(df, return_df=False):\n",
    "    df = df.copy()\n",
    "\n",
    "    # CLEAN PROVINCE COLUMN\n",
    "\n",
    "    df['Province'] = df['Province'].str.replace(' ', '_')\n",
    "    df['Province'] = df['Province'].str.replace('.', '_')\n",
    "    df['Province'] = df['Province'].str.replace('-', '_')\n",
    "\n",
    "    # DATE ENCODING\n",
    "    # Split 'Date' column into year, month and day columns\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Date'] = df['Date'].dt.strftime('%d.%m.%Y')\n",
    "    df['year'] = pd.DatetimeIndex(df['Date']).year\n",
    "    df['month'] = pd.DatetimeIndex(df['Date']).month\n",
    "    df['day'] = pd.DatetimeIndex(df['Date']).day\n",
    "\n",
    "    df = df.drop(['Date'], axis=1)\n",
    "\n",
    "    # BINARY ENCODING\n",
    "\n",
    "    df['year'] = df['year'].replace({2020: 1, 2019: 0})\n",
    "\n",
    "    # ONE-HOT ENCODING\n",
    "    for column in ['Province', 'Container']:\n",
    "        df = onehot_encode(df, column)\n",
    "\n",
    "    # ORDINAL ENCODING\n",
    "    enc = OrdinalEncoder()\n",
    "    df[['Size_Grade']] = enc.fit_transform(df[['Size_Grade']])\n",
    "\n",
    "    # REMOVE OUTLIERS from Train set\n",
    "    if 'avg_price_per_kg' in df.columns:\n",
    "\n",
    "        df = remove_outliers(df=df,\n",
    "                             columns=[\n",
    "                                 'Weight_Kg', 'Low_Price', 'High_Price',\n",
    "                                 'Sales_Total', 'Total_Qty_Sold',\n",
    "                                 'Total_Kg_Sold', 'Stock_On_Hand'\n",
    "                             ],\n",
    "                             threshold=0.0000000000001)\n",
    "\n",
    "    if return_df == True:\n",
    "        ## for training dataset\n",
    "        # REORDER COLUMNS SO THAT OUR DEPENDENT VARIABLE IS THE LAST COLUMN OF THE DATAFRAME\n",
    "        if 'avg_price_per_kg' in df.columns:\n",
    "            column_titles = [\n",
    "                col for col in df.columns if col != 'avg_price_per_kg'\n",
    "            ] + ['avg_price_per_kg']\n",
    "            df = df.reindex(columns=column_titles)\n",
    "\n",
    "        return df\n",
    "\n",
    "    ## for training dataset\n",
    "    # REORDER COLUMNS SO THAT OUR DEPENDENT VARIABLE IS THE LAST COLUMN OF THE DATAFRAME\n",
    "    elif 'avg_price_per_kg' in df.columns:\n",
    "        column_titles = [\n",
    "            col for col in df.columns if col != 'avg_price_per_kg'\n",
    "        ] + ['avg_price_per_kg']\n",
    "        df = df.reindex(columns=column_titles)\n",
    "\n",
    "        # SPLIT DATA INTO PREDICTORS AND TARGET\n",
    "\n",
    "        y = df['avg_price_per_kg']\n",
    "        X = df.drop('avg_price_per_kg', axis=1)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # TRAIN TEST SPLIT\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                            y,\n",
    "                                                            test_size=0.35,\n",
    "                                                            shuffle=False,\n",
    "                                                            random_state=seed)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_inputs(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TRAIN A FEW MODELS\n",
    "\n",
    "models = {\n",
    "    \"                         Decision Tree\": DecisionTreeRegressor(random_state= seed),\n",
    "    \"                         Random Forest\": RandomForestRegressor(min_samples_leaf= 1, n_estimators = 500, random_state= seed, max_depth = 13),\n",
    "    \"                     Gradient Boosting\": GradientBoostingRegressor(learning_rate=0.18, n_estimators=6000, random_state= seed, max_depth =2),\n",
    "    \"                               XGBoost\": XGBRegressor(max_depth=2,min_child_weight=13,subsample=1,colsample_bytree=1,\n",
    "            objective='reg:squarederror',n_estimators=6000, learning_rate=0.3, random_state= seed),\n",
    "    \"                     CatBoostRegressor\": CatBoostRegressor(verbose=0, learning_rate=0.09, depth = 4, iterations= 7000),\n",
    "    \"                         LGBMRegressor\": LGBMRegressor()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    print(name + \" trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE MODEL ON R SQUARED - HIGHER IS BETTER\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(name + \" R^2 Score: {:.5f}\".format(model.score(X_test, y_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE MODEL ON RMSE - LOWER IS BETTER\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(name + \" Test RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error(y_test ,y_pred))))\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    print(name + \" Train RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error(y_train_pred ,y_train))))\n",
    "    \n",
    "    errors = abs(y_pred - y_test)\n",
    "\n",
    "    # Display the performance metrics\n",
    "    print('Mean Absolute Error:', round(np.mean(errors), 2), 'Rand.')\n",
    "\n",
    "    mape = np.mean(100 * (errors / y_test))\n",
    "    accuracy = 100 - mape\n",
    "\n",
    "    print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEAUTURE IMPORTANCES\n",
    "\n",
    "df = train.drop('avg_price_per_kg', axis = 1)\n",
    "features = preprocess_inputs(df)\n",
    "feature_list = list(features.columns)\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name == \"                               XGBoost\":\n",
    "        # Get numerical feature importances\n",
    "        importances = list(model.feature_importances_)\n",
    "\n",
    "        # List of tuples with variable and importance\n",
    "        feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "        # Sort the feature importances by most important first\n",
    "        feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "        # Print out the feature and importances \n",
    "        [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECTING IMPORTANT FEATURES\n",
    "\n",
    "df = preprocess_inputs(train, return_df=True)\n",
    "\n",
    "\n",
    "y = df['avg_price_per_kg']\n",
    "X = df.drop('avg_price_per_kg', axis=1)\n",
    "\n",
    "# GET IMPORTANT COLUMN NAMES\n",
    "\n",
    "important = []\n",
    "for i in [ 'Total_Kg_Sold', 'Container_IA400', 'Container_M4183', \"Container_JE090\", 'Container_JG110', \n",
    "          'Weight_Kg', 'Total_Qty_Sold', 'High_Price', 'Sales_Total', 'Stock_On_Hand']:\n",
    "    A = [col for col in df.columns if i in col]\n",
    "    important.append(A)\n",
    "    \n",
    "important_list = [item for sublist in important for item in sublist]\n",
    "\n",
    "# IMPORTANT DATAFRAME\n",
    "\n",
    "X_imp = X[important_list]\n",
    "print(important_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST NEW MODEL WITH IMPORTANT FEATURES ONLY\n",
    "\n",
    "X_imp_train, X_imp_test, y_train, y_test = train_test_split(X_imp, y, test_size=0.35, shuffle=False, random_state=seed)\n",
    "\n",
    "xgb = XGBRegressor(max_depth=2,min_child_weight=13,subsample=1,colsample_bytree=1,\n",
    "            objective='reg:squarederror',n_estimators=6000, learning_rate=0.3, random_state= seed)\n",
    "xgb.fit(X_imp_train, y_train)\n",
    "print(\"Trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK PERFORMANCE METRICS\n",
    "\n",
    "pred = xgb.predict(X_imp_test)\n",
    "\n",
    "errors = abs(pred - y_test)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'rand.')\n",
    "\n",
    "mape = np.mean(100 * (errors / y_test))\n",
    "accuracy = 100 - mape\n",
    "\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "print(\"Test RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error(y_test ,pred))))\n",
    "y_train_pred = xgb.predict(X_imp_train)\n",
    "print(\"Train RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error(y_train_pred ,y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare all the models with their previous score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN MODELS AGAIN BUT WITH THE SUBSET CREATED ABOVE\n",
    "\n",
    "X_imp_train, X_imp_test, y_train, y_test = train_test_split(X_imp, y, test_size=0.35, shuffle=False, random_state=seed)\n",
    "# TRAIN A FEW MODELS\n",
    "\n",
    "models = {\n",
    "    \"                         Decision Tree\": DecisionTreeRegressor(random_state= seed),\n",
    "    \"                         Random Forest\": RandomForestRegressor(min_samples_leaf= 1, n_estimators = 500, random_state= seed, max_depth = 13),\n",
    "    \"                     Gradient Boosting\": GradientBoostingRegressor(learning_rate=0.18, n_estimators=6000, random_state= seed, max_depth =2),\n",
    "    \"                               XGBoost\": XGBRegressor(max_depth=2,min_child_weight=13,subsample=1,colsample_bytree=1,\n",
    "            objective='reg:squarederror',n_estimators=6000, learning_rate=0.3, random_state= seed),\n",
    "    \"                     CatBoostRegressor\": CatBoostRegressor(verbose=0, learning_rate=0.09, depth = 4, iterations= 7000),\n",
    "    \"                         LGBMRegressor\": LGBMRegressor()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_imp_train, y_train)\n",
    "    print(name + \" trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE SQUARED - HIGHER IS BETTER\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(name + \" R^2 Score: {:.5f}\".format(model.score(X_imp_test, y_test)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE MODEL ON RMSE\n",
    "\n",
    "X_imp_train, X_imp_test, y_train, y_test = train_test_split(X_imp, y, test_size=0.35, shuffle=False, random_state=seed)\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_imp_test)\n",
    "    print(name + \" Test RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error(y_test ,y_pred))))\n",
    "    \n",
    "    y_train_pred = model.predict(X_imp_train)\n",
    "    print(name + \" Train RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error( y_train,y_train_pred))))\n",
    "    \n",
    "    errors = abs(y_pred - y_test)\n",
    "\n",
    "    # Display the performance metrics\n",
    "    print('Mean Absolute Error:', round(np.mean(errors), 2), 'Rand.')\n",
    "\n",
    "    mape = np.mean(100 * (errors / y_test))\n",
    "    accuracy = 100 - mape\n",
    "\n",
    "    print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It apears the majority of the best performing models from our initial training have improved. Lets improve them a bit more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW ENSEMBLE STACKING\n",
    "\n",
    "# CHOOSE BEST MODELS FROM EARLIER SCORES\n",
    "\n",
    "gb = GradientBoostingRegressor(learning_rate=0.18, n_estimators=6000, random_state= seed, max_depth =2)\n",
    "xgb = XGBRegressor(max_depth=2,min_child_weight=13,subsample=1,colsample_bytree=1,\n",
    "            objective='reg:squarederror',n_estimators=6000, learning_rate=0.3, random_state= seed)\n",
    "meta_learner_reg =  XGBRegressor(max_depth=2,min_child_weight=13,subsample=1,colsample_bytree=1, \n",
    "                                 objective='reg:squarederror', n_estimators=6000, learning_rate=0.3, random_state= seed)\n",
    "\n",
    "models_4stacking = [(\"gb\", gb),(\"xgb\", xgb)]\n",
    "\n",
    "s_reg = StackingRegressor(estimators=models_4stacking, final_estimator= meta_learner_reg, passthrough = True, cv= 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN ON DATAFRAME WITH 7 COLUMNS(MOST IMPORTANT)\n",
    "\n",
    "X_imp_train, X_imp_test, y_train, y_test = train_test_split(X_imp, y, test_size=0.35, shuffle=False, random_state=seed)\n",
    "\n",
    "s_reg.fit(X_imp_train,y_train)\n",
    "print(\"Stacked model fitted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_imp_train, X_imp_test, y_train, y_test = train_test_split(X_imp, y, test_size=0.35, shuffle=False, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# METRICS OF STACKING REGRESSOR\n",
    "\n",
    "y_pred = s_reg.predict(X_imp_test)\n",
    "rsq = s_reg.score(X_imp_test, y_test)\n",
    "print(\"R^2 Score: \", rsq)\n",
    "\n",
    "print(\"Test RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error(y_test ,y_pred))))\n",
    "\n",
    "y_train_pred = model.predict(X_imp_train)\n",
    "print(\"Train RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error(y_train_pred ,y_train))))\n",
    "errors = abs(y_pred - y_test)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'Rand.')\n",
    "mape = np.mean(100 * (errors / y_test))\n",
    "accuracy = 100 - mape\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING STACKED REGRESSOR PREDICTIONS to CSV\n",
    "\n",
    "df = preprocess_inputs(test)\n",
    "Xs = list(df.columns)\n",
    "Xs.remove('Index')\n",
    "\n",
    "X_test = df[Xs]\n",
    "\n",
    "x_t = X_test[important_list]\n",
    "\n",
    "y_pred = s_reg.predict(x_t)\n",
    "d = pd.DataFrame(y_pred, columns =['avg_price_per_kg'])\n",
    "dff = pd.concat([df['Index'], d], axis=1)\n",
    "dff = dff.set_index('Index')\n",
    "dff.to_csv('StackeD.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING KFOLD\n",
    "\n",
    "df = preprocess_inputs(train, return_df=True)\n",
    "\n",
    "y = df['avg_price_per_kg']\n",
    "X = df.drop('avg_price_per_kg', axis=1)\n",
    "\n",
    "important = []\n",
    "for i in [ 'Total_Kg_Sold', 'Container_IA400', 'Container_M4183', \"Container_JE090\", 'Container_JG110', \n",
    "          'Weight_Kg', 'Total_Qty_Sold', 'High_Price', 'Sales_Total', 'Stock_On_Hand']:\n",
    "    A = [col for col in df.columns if i in col]\n",
    "    important.append(A)\n",
    "    \n",
    "important_list = [item for sublist in important for item in sublist]\n",
    "\n",
    "# IMPORTANT DATAFRAME\n",
    "X_imp = X[important_list]\n",
    "\n",
    "def split_data_kf(df,K):\n",
    "    \n",
    "    y = df['avg_price_per_kg']\n",
    "    X = df[important_list]  # SUBSET FOR IMPORTANT ROWS\n",
    "    \n",
    "    kf = KFold(n_splits=K, shuffle = False)\n",
    "    indices = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        indices.append((train_index, test_index))\n",
    "        \n",
    "    return indices\n",
    "\n",
    "\n",
    "def get_best_kfmodel(df,data_indices):\n",
    "    y = df['avg_price_per_kg']\n",
    "    X = df[important_list]\n",
    "    \n",
    "    RMSE = []\n",
    "    trainRMSE =[]\n",
    "    \n",
    "    for (train_indices,test_indices) in data_indices:\n",
    "        X_train, y_train = X.iloc[train_indices,:],y.iloc[train_indices]\n",
    "        X_test, y_test = X.iloc[test_indices,:], y.iloc[test_indices]\n",
    "        \n",
    "        model = StackingRegressor(estimators=models_4stacking, final_estimator= meta_learner_reg, passthrough = True, cv= 4)\n",
    "        \n",
    "        model.fit(X_train.values, y_train.values) \n",
    "\n",
    "        y_pred = model.predict(X_test.values)     \n",
    "        \n",
    "        rmse = np.sqrt(metrics.mean_squared_error(y_test.values ,y_pred))\n",
    "        RMSE.append(rmse)\n",
    "        \n",
    "        y_train_pred = model.predict(X_train.values)\n",
    "        \n",
    "        train_rsme = np.sqrt(metrics.mean_squared_error( y_train.values,y_train_pred))\n",
    "        trainRMSE.append(train_rsme)\n",
    "        \n",
    "    best = RMSE.index(min(RMSE))\n",
    "    best_indicies = data_indices[best]\n",
    "    \n",
    "    \n",
    "    X_train, y_train = X.iloc[best_indicies[0],:],y.iloc[best_indicies[0]]\n",
    "    X_test, y_test = X.iloc[best_indicies[1],:], y.iloc[best_indicies[1]]\n",
    "    \n",
    "    model = StackingRegressor(estimators=models_4stacking, final_estimator= meta_learner_reg, passthrough = True, cv= 4)\n",
    "    \n",
    "    model.fit(X_train.values, y_train.values)       \n",
    "          \n",
    "    return model\n",
    "\n",
    "\n",
    "    \n",
    "# GET THE KFOLD SPLIT ON WHICH THE MODEL WAS TRAINED\n",
    "def get_best_split(df,data_indices, model):\n",
    "    y = df['avg_price_per_kg']\n",
    "    X = df[important_list]\n",
    "    \n",
    "    RMSE = []\n",
    "    trainRMSE =[]\n",
    "    \n",
    "    for (train_indices,test_indices) in data_indices:\n",
    "        X_train, y_train = X.iloc[train_indices,:],y.iloc[train_indices]\n",
    "        X_test, y_test = X.iloc[test_indices,:], y.iloc[test_indices]\n",
    "\n",
    "        y_pred = model.predict(X_test.values)     \n",
    "        \n",
    "        rmse = np.sqrt(metrics.mean_squared_error(y_test.values ,y_pred))\n",
    "        RMSE.append(rmse)\n",
    "        \n",
    "        y_train_pred = model.predict(X_train.values)\n",
    "        \n",
    "        train_rsme = np.sqrt(metrics.mean_squared_error( y_train.values,y_train_pred))\n",
    "        trainRMSE.append(train_rsme)\n",
    "        \n",
    "    best = RMSE.index(min(RMSE))\n",
    "    best_indicies = data_indices[best]\n",
    "    \n",
    "    X_train, y_train = X.iloc[best_indicies[0],:],y.iloc[best_indicies[0]]\n",
    "    X_test, y_test = X.iloc[best_indicies[1],:], y.iloc[best_indicies[1]]\n",
    "          \n",
    "    return X_train.values, X_test.values, y_train.values, y_test.values\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Find our best model\n",
    "\n",
    "train_df = preprocess_inputs(train, return_df = True)\n",
    "data_indices = split_data_kf(train_df,4)\n",
    "model = get_best_kfmodel(train_df,data_indices)\n",
    "\n",
    "print('Best Stacking Regressor Trained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets save our model\n",
    "#  #Pickling\n",
    "\n",
    "model_save_path = \"assets/trained-models/stacked_kfoldarr.pkl\"\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(model,file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle the model\n",
    "\n",
    "model_load_path = \"assets/trained-models/stacked_kfoldarr.pkl\"\n",
    "with open(model_load_path,'rb') as file:\n",
    "    unpickled_model = pickle.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET METRICS OF STACKING REGRESSOR MODEL\n",
    "\n",
    "df = preprocess_inputs(train, return_df = True)\n",
    "data_indices = split_data_kf(df,4)\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_best_split(df, data_indices, unpickled_model)\n",
    "s_reg = unpickled_model \n",
    "\n",
    "y_pred = s_reg.predict(X_test)\n",
    "rsq = s_reg.score(X_test, y_test)\n",
    "print(\"R^2 Score: \", rsq)\n",
    "\n",
    "print(\"Test RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error(y_test ,y_pred))))\n",
    "\n",
    "y_train_pred = s_reg.predict(X_train)\n",
    "print(\"Train RMSE: {:.5f}\".format(np.sqrt(metrics.mean_squared_error(y_train_pred ,y_train))))\n",
    "errors = abs(y_pred - y_test)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'Rand.')\n",
    "mape = np.mean(100 * (errors / y_test))\n",
    "accuracy = 100 - mape\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with best model\n",
    "df = preprocess_inputs(test)\n",
    "Xs = list(df.columns)\n",
    "Xs.remove('Index')\n",
    "\n",
    "X_test = df[Xs]\n",
    "X_Test_imp = X_test[important_list]\n",
    "\n",
    "y_pred = unpickled_model.predict(X_Test_imp.values)\n",
    "d = pd.DataFrame(y_pred, columns =['avg_price_per_kg'])\n",
    "dff = pd.concat([df['Index'], d], axis=1)\n",
    "dff = dff.set_index('Index')\n",
    "dff.to_csv('stackedkfoldarr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess model accuracy with plot of the predicted y values against the actual y values from x test split\n",
    "# Stacking Regressor of Gradient Boosting and XGBooster Regressor - our best model\n",
    "\n",
    "df = preprocess_inputs(train, return_df = True)\n",
    "data_indices = split_data_kf(df,4)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imp, y, test_size=0.35, shuffle=False, random_state=seed)\n",
    "s_reg = unpickled_model \n",
    "\n",
    "y_pred = s_reg.predict(X_test)\n",
    "rsq = round(s_reg.score(X_test, y_test), 5)\n",
    "test_rmse = round(np.sqrt(metrics.mean_squared_error(y_test ,y_pred)), 5)\n",
    "y_train_pred = s_reg.predict(X_train)\n",
    "train_rmse = round(np.sqrt(metrics.mean_squared_error(y_train_pred ,y_train)), 5)\n",
    "\n",
    "errors = abs(y_pred - y_test)\n",
    "MAE = round(np.mean(errors), 2)\n",
    "mape = np.mean(100 * (errors / y_test))\n",
    "accuracy = round((100 - mape), 2)\n",
    "\n",
    "\n",
    "stats = f\"test RMSE = {test_rmse} \\n \" + \\\n",
    "          f\"train RMSE = {train_rmse} \\n\" + \\\n",
    "          f\"R^2 Score = {rsq} \\n\" + \\\n",
    "         f\"MAE = {MAE} \\n\" + \\\n",
    "         f\"Accuracy = {accuracy}\" \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "ax.plot(y_test, y_test, 'r')\n",
    "g=ax.scatter(y_test, y_pred, s = 9)\n",
    "g.axes.set_xlabel('True Values')\n",
    "g.axes.set_ylabel('Predictions')\n",
    "g.axes.set_title('Predictions vs Actual\\n \\\n",
    "                Stacking Regressor of \\\n",
    "                 \\n Gradient Boosting and XGBooster Regressor\\n \\n' + stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets compare the performance of this stacked model to the individual models performances \n",
    "\n",
    "#first lets train them again\n",
    "\n",
    "df = preprocess_inputs(train, return_df=True)\n",
    "y = df['avg_price_per_kg']\n",
    "X = df.drop('avg_price_per_kg', axis=1)\n",
    "important_list = ['Total_Kg_Sold', 'Container_IA400', 'Container_M4183', 'Container_JE090', \n",
    " 'Container_JG110', 'Weight_Kg', 'Total_Qty_Sold', 'High_Price', 'Sales_Total', 'Stock_On_Hand']\n",
    "X_imp = X[important_list]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imp, y, test_size=0.35, shuffle=False, random_state=seed)\n",
    "\n",
    "\n",
    "gb = GradientBoostingRegressor(learning_rate=0.18, n_estimators=6000, random_state= seed, max_depth =2)\n",
    "xgb = XGBRegressor(max_depth=2,min_child_weight=13,subsample=1,colsample_bytree=1,\n",
    "            objective='reg:squarederror',n_estimators=6000, learning_rate=0.3, random_state= seed)\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "gb.fit(X_train, y_train)\n",
    "print('Trained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess model accuracy with plot of the predicted y values against the actual y values from x test split\n",
    "# Gradient Boosting Regressor\n",
    "\n",
    "model = gb\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "rsq = round(model.score(X_test, y_test), 5)\n",
    "test_rmse = round(np.sqrt(metrics.mean_squared_error(y_test ,y_pred)), 5)\n",
    "y_train_pred = model.predict(X_train)\n",
    "train_rmse = round(np.sqrt(metrics.mean_squared_error(y_train_pred ,y_train)), 5)\n",
    "\n",
    "errors = abs(y_pred - y_test)\n",
    "MAE = round(np.mean(errors), 2)\n",
    "mape = np.mean(100 * (errors / y_test))\n",
    "accuracy = round((100 - mape), 2)\n",
    "\n",
    "stats = f\"test RMSE = {test_rmse} \\n \" + \\\n",
    "          f\"train RMSE = {train_rmse} \\n\" + \\\n",
    "          f\"R^2 Score = {rsq} \\n\" + \\\n",
    "         f\"MAE = {MAE} \\n\" + \\\n",
    "         f\"Accuracy = {accuracy}\" \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "ax.plot(y_test, y_test, 'r')\n",
    "g=ax.scatter(y_test, y_pred, s = 9)\n",
    "g.axes.set_xlabel('True Values')\n",
    "g.axes.set_ylabel('Predictions')\n",
    "g.axes.set_title('Predictions vs Actual \\n Gradient Boosting Regressor\\n \\n' + stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess model accuracy with plot of the predicted y values against the actual y values from x test split\n",
    "# XGBooster Regressor\n",
    "\n",
    "model = xgb\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "rsq = round(model.score(X_test, y_test), 5)\n",
    "test_rmse = round(np.sqrt(metrics.mean_squared_error(y_test ,y_pred)), 5)\n",
    "y_train_pred = model.predict(X_train)\n",
    "train_rmse = round(np.sqrt(metrics.mean_squared_error(y_train_pred ,y_train)), 5)\n",
    "\n",
    "errors = abs(y_pred - y_test)\n",
    "MAE = round(np.mean(errors), 2)\n",
    "mape = np.mean(100 * (errors / y_test))\n",
    "accuracy = round((100 - mape), 2)\n",
    "\n",
    "\n",
    "stats = f\"test RMSE = {test_rmse} \\n \" + \\\n",
    "          f\"train RMSE = {train_rmse} \\n\" + \\\n",
    "          f\"R^2 Score = {rsq} \\n\" + \\\n",
    "         f\"MAE = {MAE} \\n\" + \\\n",
    "         f\"Accuracy = {accuracy}\" \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "ax.plot(y_test, y_test, 'r')\n",
    "g=ax.scatter(y_test, y_pred, s = 9)\n",
    "g.axes.set_xlabel('True Values')\n",
    "g.axes.set_ylabel('Predictions')\n",
    "g.axes.set_title('Predictions vs Actual \\n XGBooster Regressor\\n \\n' + stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
